{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics: Prerequisites for Neural Networks\n",
    "\n",
    "Before diving into more complex neural network architectures, let's understand PyTorch - the deep learning framework we'll use for the rest of the course.\n",
    "\n",
    "## Table of Contents\n",
    "1. [What is PyTorch?](#1)\n",
    "2. [Tensors: The Building Block](#2)\n",
    "3. [Tensors vs NumPy Arrays](#3)\n",
    "4. [Basic Tensor Operations](#4)\n",
    "5. [Autograd: Automatic Differentiation](#5)\n",
    "6. [Building Neural Networks with PyTorch](#6)\n",
    "7. [GPU Acceleration](#7)\n",
    "8. [Summary](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is PyTorch?<a id=\"1\"></a>\n",
    "\n",
    "PyTorch is a deep learning framework developed by Meta (Facebook). Think of it as:\n",
    "- **NumPy on steroids** - similar array operations but with GPU support\n",
    "- **Automatic differentiation engine** - like the micrograd you just built, but production-ready\n",
    "- **Neural network building blocks** - pre-built layers, optimizers, and utilities\n",
    "\n",
    "After building micrograd, you understand the core concepts. PyTorch is essentially a highly optimized, industrial-strength version of what you just created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/60/d4z7jcx15cxfnm_vstk_wvl00000gn/T/ipykernel_85849/973751702.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/brunomakoto/anaconda3/envs/sklearn_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensors: The Building Block<a id=\"2\"></a>\n",
    "\n",
    "### What is a Tensor?\n",
    "\n",
    "A **tensor** is a multi-dimensional array - a generalization of scalars, vectors, and matrices:\n",
    "\n",
    "- **Scalar** (0D tensor): `5`\n",
    "- **Vector** (1D tensor): `[1, 2, 3]`\n",
    "- **Matrix** (2D tensor): `[[1, 2], [3, 4]]`\n",
    "- **3D+ tensor**: Higher dimensional arrays\n",
    "\n",
    "In deep learning:\n",
    "- Images are typically **4D tensors**: `(batch_size, channels, height, width)`\n",
    "- Text sequences are **3D tensors**: `(batch_size, sequence_length, embedding_dim)`\n",
    "- Video data are **5D tensors**: `(batch_size, frames, channels, height, width)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors\n",
    "scalar = torch.tensor(5)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "matrix = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "\n",
    "print(f\"Scalar shape: {scalar.shape}, ndim: {scalar.ndim}\")\n",
    "print(f\"Vector shape: {vector.shape}, ndim: {vector.ndim}\")\n",
    "print(f\"Matrix shape: {matrix.shape}, ndim: {matrix.ndim}\")\n",
    "print(f\"3D Tensor shape: {tensor_3d.shape}, ndim: {tensor_3d.ndim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tensors vs NumPy Arrays<a id=\"3\"></a>\n",
    "\n",
    "### Why use PyTorch tensors instead of NumPy?\n",
    "\n",
    "#### 1. **GPU Acceleration**\n",
    "NumPy only runs on CPU. PyTorch tensors can run on GPU, giving you **10-100x speedup** for large operations.\n",
    "\n",
    "#### 2. **Automatic Differentiation**\n",
    "PyTorch tracks operations and can automatically compute gradients - essential for training neural networks. Remember the backpropagation you implemented in micrograd? PyTorch does this automatically!\n",
    "\n",
    "#### 3. **Deep Learning Ecosystem**\n",
    "PyTorch integrates seamlessly with neural network layers, optimizers, and other deep learning tools.\n",
    "\n",
    "#### 4. **Dynamic Computation Graphs**\n",
    "PyTorch builds computation graphs on-the-fly, making debugging easier and allowing for dynamic architectures.\n",
    "\n",
    "### When to use what?\n",
    "- **NumPy**: Data preprocessing, traditional scientific computing, when you don't need gradients or GPU\n",
    "- **PyTorch**: Training neural networks, operations requiring gradients, when you need GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting between NumPy and PyTorch\n",
    "np_array = np.array([1, 2, 3, 4, 5])\n",
    "torch_tensor = torch.from_numpy(np_array)\n",
    "\n",
    "print(f\"NumPy array: {np_array}\")\n",
    "print(f\"PyTorch tensor: {torch_tensor}\")\n",
    "\n",
    "# Convert back to NumPy\n",
    "back_to_numpy = torch_tensor.numpy()\n",
    "print(f\"Back to NumPy: {back_to_numpy}\")\n",
    "\n",
    "# They share memory! Changing one affects the other\n",
    "np_array[0] = 999\n",
    "print(f\"After modifying np_array, torch_tensor: {torch_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar API to NumPy\n",
    "print(\"\\nPyTorch has similar operations to NumPy:\")\n",
    "print(f\"torch.zeros(3, 3):\\n{torch.zeros(3, 3)}\\n\")\n",
    "print(f\"torch.ones(2, 4):\\n{torch.ones(2, 4)}\\n\")\n",
    "print(f\"torch.randn(2, 3): # Normal distribution\\n{torch.randn(2, 3)}\\n\")\n",
    "print(f\"torch.arange(0, 10, 2): {torch.arange(0, 10, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Tensor Operations<a id=\"4\"></a>\n",
    "\n",
    "PyTorch operations are very similar to NumPy, so if you know NumPy, you're already 80% there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise operations\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "y = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "print(f\"x + y = {x + y}\")\n",
    "print(f\"x * y = {x * y}\")\n",
    "print(f\"x ** 2 = {x ** 2}\")\n",
    "print(f\"torch.sqrt(x) = {torch.sqrt(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication shape: torch.Size([3, 4]) @ torch.Size([4, 2]) = torch.Size([3, 2])\n",
      "Result:\n",
      "tensor([[-1.1068,  0.7997],\n",
      "        [-1.8041,  0.7852],\n",
      "        [-0.5516,  0.5687]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix operations\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 2)\n",
    "\n",
    "# Matrix multiplication\n",
    "C = torch.matmul(A, B)  # or A @ B\n",
    "print(f\"Matrix multiplication shape: {A.shape} @ {B.shape} = {C.shape}\")\n",
    "print(f\"Result:\\n{C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "Reshaped to (3, 4):\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "Reshaped to (2, 6):\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n",
      "\n",
      "First row: tensor([0, 1, 2, 3])\n",
      "First column: tensor([0, 4, 8])\n",
      "Submatrix:\n",
      "tensor([[0, 1],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# Reshaping and indexing\n",
    "x = torch.arange(12)\n",
    "print(f\"Original: {x}\")\n",
    "print(f\"Reshaped to (3, 4):\\n{x.reshape(3, 4)}\")\n",
    "print(f\"Reshaped to (2, 6):\\n{x.reshape(2, 6)}\")\n",
    "\n",
    "# Indexing works like NumPy\n",
    "matrix = x.reshape(3, 4)\n",
    "print(f\"\\nFirst row: {matrix[0]}\")\n",
    "print(f\"First column: {matrix[:, 0]}\")\n",
    "print(f\"Submatrix:\\n{matrix[:2, :2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([1, 3])\n",
      "y shape: torch.Size([3, 1])\n",
      "x + y (broadcasted):\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [4, 5, 6]])\n",
      "Result shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting - automatic expansion of dimensions\n",
    "x = torch.tensor([[1, 2, 3]])\n",
    "y = torch.tensor([[1], [2], [3]])\n",
    "\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"x + y (broadcasted):\\n{x + y}\")\n",
    "print(f\"Result shape: {(x + y).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Autograd: Automatic Differentiation<a id=\"5\"></a>\n",
    "\n",
    "This is where PyTorch really shines! Remember in micrograd how you manually implemented the `backward()` method for each operation? PyTorch does this automatically.\n",
    "\n",
    "### Key Concepts:\n",
    "- `requires_grad=True`: Tells PyTorch to track operations on this tensor\n",
    "- `.backward()`: Computes all gradients automatically\n",
    "- `.grad`: Stores the gradient after calling `.backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 3.0\n",
      "y = x^2 = 9.0\n",
      "dy/dx = 6.0\n"
     ]
    }
   ],
   "source": [
    "# Simple example: f(x) = x^2\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"y = x^2 = {y.item()}\")\n",
    "\n",
    "# Compute gradient\n",
    "y.backward()\n",
    "\n",
    "# dy/dx = 2x = 2*3 = 6\n",
    "print(f\"dy/dx = {x.grad.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=2.0, b=3.0\n",
      "c = a + b = 5.0\n",
      "d = a - b = -1.0\n",
      "e = c * d = -5.0\n",
      "\n",
      "de/da = 4.0\n",
      "de/db = -6.0\n"
     ]
    }
   ],
   "source": [
    "# More complex example - just like micrograd!\n",
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# f(a,b) = (a + b) * (a - b)\n",
    "c = a + b  # c = 5\n",
    "d = a - b  # d = -1\n",
    "e = c * d  # e = -5\n",
    "\n",
    "print(f\"a={a.item()}, b={b.item()}\")\n",
    "print(f\"c = a + b = {c.item()}\")\n",
    "print(f\"d = a - b = {d.item()}\")\n",
    "print(f\"e = c * d = {e.item()}\")\n",
    "\n",
    "# Compute gradients\n",
    "e.backward()\n",
    "\n",
    "print(f\"\\nde/da = {a.grad.item()}\")\n",
    "print(f\"de/db = {b.grad.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 2.6000\n",
      "Loss: 0.3600\n",
      "\n",
      "Gradients:\n",
      "dL/dw = tensor([1.2000, 2.4000, 3.6000])\n",
      "dL/db = 1.2000\n"
     ]
    }
   ],
   "source": [
    "# Neural network example - forward pass with autograd\n",
    "# Simulating: y = w * x + b\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "w = torch.tensor([0.5, -0.5, 1.0], requires_grad=True)\n",
    "b = torch.tensor(0.1, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "y_pred = (w * x).sum() + b\n",
    "y_true = torch.tensor(2.0)\n",
    "\n",
    "# Loss (mean squared error)\n",
    "loss = (y_pred - y_true) ** 2\n",
    "\n",
    "print(f\"Prediction: {y_pred.item():.4f}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nGradients:\")\n",
    "print(f\"dL/dw = {w.grad}\")\n",
    "print(f\"dL/db = {b.grad.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After first backward: x.grad = 4.0\n",
      "After second backward (accumulated): x.grad = 16.0\n",
      "After zeroing: x.grad = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Important: Gradient accumulation\n",
    "# Gradients accumulate by default - you need to zero them!\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# First computation\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print(f\"After first backward: x.grad = {x.grad.item()}\")\n",
    "\n",
    "# Second computation without zeroing\n",
    "y = x ** 3\n",
    "y.backward()\n",
    "print(f\"After second backward (accumulated): x.grad = {x.grad.item()}\")\n",
    "\n",
    "# Zero the gradients\n",
    "x.grad.zero_()\n",
    "print(f\"After zeroing: x.grad = {x.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Micrograd\n",
    "\n",
    "In micrograd, you did:\n",
    "```python\n",
    "class Value:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "```\n",
    "\n",
    "PyTorch tensors work the same way, but:\n",
    "- More efficient (optimized C++/CUDA code)\n",
    "- Handles arrays/tensors, not just scalars\n",
    "- Automatic graph construction and differentiation\n",
    "- GPU support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building Neural Networks with PyTorch<a id=\"6\"></a>\n",
    "\n",
    "PyTorch provides `torch.nn` module with pre-built layers and utilities. This is like your `Neuron`, `Layer`, and `MLP` classes in micrograd, but production-ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMLP(\n",
      "  (layer1): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (layer2): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "Input shape: torch.Size([5, 10])\n",
      "Output shape: torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Simple neural network - similar to your MLP in micrograd\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))  # Hidden layer with ReLU\n",
    "        x = self.layer2(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = SimpleMLP(input_size=10, hidden_size=20, output_size=3)\n",
    "print(model)\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(5, 10)  # Batch of 5 samples, 10 features each\n",
    "output = model(x)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\n",
      "layer1.weight: shape torch.Size([20, 10]), requires_grad=True\n",
      "layer1.bias: shape torch.Size([20]), requires_grad=True\n",
      "layer2.weight: shape torch.Size([3, 20]), requires_grad=True\n",
      "layer2.bias: shape torch.Size([3]), requires_grad=True\n",
      "\n",
      "Total parameters: 283\n"
     ]
    }
   ],
   "source": [
    "# Access model parameters (like your .parameters() method in micrograd)\n",
    "print(\"Model parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: shape {param.shape}, requires_grad={param.requires_grad}\")\n",
    "\n",
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.1178\n",
      "Epoch 2, Loss: 1.1165\n",
      "Epoch 3, Loss: 1.1152\n",
      "Epoch 4, Loss: 1.1139\n",
      "Epoch 5, Loss: 1.1127\n"
     ]
    }
   ],
   "source": [
    "# Training loop example - putting it all together\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create simple dataset\n",
    "X = torch.randn(100, 10)  # 100 samples\n",
    "y = torch.randint(0, 3, (100,))  # 100 labels (3 classes)\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "model = SimpleMLP(10, 20, 3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.025)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()  # Zero gradients (like x.grad.zero_())\n",
    "    loss.backward()  # Compute gradients\n",
    "    optimizer.step()  # Update weights\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Micrograd\n",
    "\n",
    "**Micrograd:**\n",
    "```python\n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        self.layers = [Layer(nin, nouts[0])] + [Layer(nouts[i], nouts[i+1]) for i in range(len(nouts)-1)]\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "```\n",
    "\n",
    "**PyTorch:**\n",
    "```python\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, nin, nouts):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(nin, nouts[0])] + [nn.Linear(nouts[i], nouts[i+1]) for i in range(len(nouts)-1)])\n",
    "    \n",
    "    # parameters() is inherited from nn.Module!\n",
    "```\n",
    "\n",
    "Same concepts, just more powerful and efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. GPU Acceleration<a id=\"7\"></a>\n",
    "\n",
    "One of the biggest advantages of PyTorch over NumPy is GPU support. Deep learning operations are highly parallelizable, and GPUs can give you **10-100x speedup**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving tensors to GPU\n",
    "x_cpu = torch.randn(1000, 1000)\n",
    "print(f\"Tensor on: {x_cpu.device}\")\n",
    "\n",
    "# Move to GPU (if available)\n",
    "x_gpu = x_cpu.to(device)\n",
    "print(f\"Tensor on: {x_gpu.device}\")\n",
    "\n",
    "# Or create directly on GPU\n",
    "y_gpu = torch.randn(1000, 1000, device=device)\n",
    "print(f\"Created directly on: {y_gpu.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed comparison (only if GPU is available)\n",
    "if torch.cuda.is_available():\n",
    "    import time\n",
    "    \n",
    "    size = 5000\n",
    "    \n",
    "    # CPU\n",
    "    x_cpu = torch.randn(size, size)\n",
    "    y_cpu = torch.randn(size, size)\n",
    "    \n",
    "    start = time.time()\n",
    "    z_cpu = torch.matmul(x_cpu, y_cpu)\n",
    "    cpu_time = time.time() - start\n",
    "    \n",
    "    # GPU\n",
    "    x_gpu = x_cpu.to(device)\n",
    "    y_gpu = y_cpu.to(device)\n",
    "    \n",
    "    # Warm up GPU\n",
    "    _ = torch.matmul(x_gpu, y_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    start = time.time()\n",
    "    z_gpu = torch.matmul(x_gpu, y_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_time = time.time() - start\n",
    "    \n",
    "    print(f\"CPU time: {cpu_time:.4f}s\")\n",
    "    print(f\"GPU time: {gpu_time:.4f}s\")\n",
    "    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "else:\n",
    "    print(\"No GPU available for speed comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving models to GPU\n",
    "model = SimpleMLP(10, 20, 3)\n",
    "model = model.to(device)\n",
    "\n",
    "# Now all inputs must be on the same device\n",
    "x = torch.randn(5, 10, device=device)\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Input device: {x.device}\")\n",
    "print(f\"Output device: {output.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary<a id=\"8\"></a>\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Tensors** are multi-dimensional arrays - the fundamental data structure in PyTorch\n",
    "\n",
    "2. **Why PyTorch over NumPy?**\n",
    "   - GPU acceleration (10-100x faster)\n",
    "   - Automatic differentiation (no manual backprop!)\n",
    "   - Deep learning ecosystem (pre-built layers, optimizers, utilities)\n",
    "   - Dynamic computation graphs (easier debugging)\n",
    "\n",
    "3. **Autograd** - PyTorch automatically tracks operations and computes gradients\n",
    "   - `requires_grad=True` to track\n",
    "   - `.backward()` to compute gradients\n",
    "   - `.grad` to access gradients\n",
    "   - Always zero gradients before next backward pass!\n",
    "\n",
    "4. **Building Neural Networks**\n",
    "   - Inherit from `nn.Module`\n",
    "   - Define layers in `__init__`\n",
    "   - Implement `forward()` method\n",
    "   - Use built-in layers like `nn.Linear`, `nn.Conv2d`, etc.\n",
    "\n",
    "5. **Training Loop Pattern**\n",
    "   ```python\n",
    "   for epoch in range(num_epochs):\n",
    "       outputs = model(inputs)          # Forward pass\n",
    "       loss = criterion(outputs, labels) # Compute loss\n",
    "       optimizer.zero_grad()             # Zero gradients\n",
    "       loss.backward()                   # Backward pass\n",
    "       optimizer.step()                  # Update weights\n",
    "   ```\n",
    "\n",
    "6. **GPU Acceleration**\n",
    "   - Move tensors and models to GPU with `.to(device)`\n",
    "   - Massive speedup for large operations\n",
    "   - Always keep tensors on the same device\n",
    "\n",
    "### Connection to Micrograd:\n",
    "\n",
    "| Micrograd | PyTorch |\n",
    "|-----------|----------|\n",
    "| `Value` | `torch.Tensor` |\n",
    "| `.data` | `.data` or `.item()` |\n",
    "| `.grad` | `.grad` |\n",
    "| `.backward()` | `.backward()` |\n",
    "| `Neuron/Layer/MLP` | `nn.Linear/nn.Module` |\n",
    "| `.parameters()` | `.parameters()` |\n",
    "\n",
    "You built micrograd to understand the fundamentals. Now PyTorch lets you scale that understanding to real-world deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand PyTorch basics, you're ready to move on to more complex architectures:\n",
    "- **makemore (Bigrams)**: Character-level language modeling\n",
    "- **makemore (MLP)**: Building deeper networks\n",
    "- **GPT**: Transformers and attention mechanisms\n",
    "\n",
    "You'll see these same PyTorch patterns everywhere:\n",
    "1. Define model architecture (`nn.Module`)\n",
    "2. Forward pass (compute predictions)\n",
    "3. Compute loss\n",
    "4. Backward pass (compute gradients)\n",
    "5. Update weights (optimizer step)\n",
    "\n",
    "Let's go! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
